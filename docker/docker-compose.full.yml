version: '3.8'

# Full deployment - All features
# LLM + Embedding required

services:
  # Neo4j Database
  neo4j:
    image: neo4j:5.15-community
    container_name: codebase-rag-neo4j-full
    ports:
      - "${NEO4J_HTTP_PORT:-7474}:7474"
      - "${NEO4J_BOLT_PORT:-7687}:7687"
    environment:
      - NEO4J_AUTH=${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-password}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=512m
    volumes:
      - neo4j_full_data:/data
      - neo4j_full_logs:/logs
      - neo4j_full_import:/var/lib/neo4j/import
      - neo4j_full_plugins:/plugins
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u ${NEO4J_USER:-neo4j} -p ${NEO4J_PASSWORD:-password} 'RETURN 1' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - codebase-rag-full
    restart: unless-stopped

  # Ollama (optional, for local LLM)
  ollama:
    image: ollama/ollama:latest
    container_name: codebase-rag-ollama-full
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_full_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - codebase-rag-full
    restart: unless-stopped
    profiles:
      - with-ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # MCP Server - Full mode
  mcp:
    build:
      context: ..
      dockerfile: docker/Dockerfile.full
    image: royisme/codebase-rag:full
    container_name: codebase-rag-mcp-full
    ports:
      - "${APP_PORT:-8000}:8000"
    environment:
      # Application
      - APP_NAME=${APP_NAME:-Code Graph Knowledge System}
      - DEBUG=${DEBUG:-false}
      - HOST=0.0.0.0
      - PORT=8000

      # Deployment mode
      - DEPLOYMENT_MODE=full
      - ENABLE_KNOWLEDGE_RAG=true
      - ENABLE_AUTO_EXTRACTION=true
      - ENABLE_MEMORY_SEARCH=true
      - ENABLE_MONITORING=${ENABLE_MONITORING:-true}

      # Neo4j Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      - NEO4J_DATABASE=${NEO4J_DATABASE:-neo4j}

      # LLM Provider
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-ollama}

      # Ollama Configuration (for with-ollama profile)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}

      # OpenAI Configuration (alternative)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      - OPENAI_EMBEDDING_MODEL=${OPENAI_EMBEDDING_MODEL:-text-embedding-3-small}

      # Gemini Configuration (alternative)
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-pro}
      - GEMINI_EMBEDDING_MODEL=${GEMINI_EMBEDDING_MODEL:-models/embedding-001}

      # OpenRouter Configuration (alternative)
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-openai/gpt-3.5-turbo}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}

      # Model Parameters
      - TEMPERATURE=${TEMPERATURE:-0.1}
      - MAX_TOKENS=${MAX_TOKENS:-2048}

      # RAG Settings
      - CHUNK_SIZE=${CHUNK_SIZE:-512}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-50}
      - TOP_K=${TOP_K:-10}
      - VECTOR_DIMENSION=${VECTOR_DIMENSION:-384}

      # Timeouts
      - CONNECTION_TIMEOUT=${CONNECTION_TIMEOUT:-30}
      - OPERATION_TIMEOUT=${OPERATION_TIMEOUT:-300}
      - LARGE_DOCUMENT_TIMEOUT=${LARGE_DOCUMENT_TIMEOUT:-600}
    volumes:
      - ${REPOS_PATH:-./repos}:/repos
      - ./data:/data
      - ./logs:/app/logs
    depends_on:
      neo4j:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - codebase-rag-full
    restart: unless-stopped

volumes:
  neo4j_full_data:
    driver: local
  neo4j_full_logs:
    driver: local
  neo4j_full_import:
    driver: local
  neo4j_full_plugins:
    driver: local
  ollama_full_data:
    driver: local

networks:
  codebase-rag-full:
    driver: bridge
