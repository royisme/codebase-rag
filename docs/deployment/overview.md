# Deployment Overview

Choose the right deployment mode based on your needs and available infrastructure.

## ğŸ¯ Deployment Modes

### Minimal - Code Graph Only

**Perfect for**: Developers who want code intelligence without LLM overhead

```yaml
Requirements:
  - Neo4j database
  - Docker & docker-compose
  - No LLM needed âœ“
  - No embedding model needed âœ“

Resources:
  - Image size: ~500MB
  - Memory: ~1GB RAM
  - Startup time: ~5 seconds
```

**Available Features**:

- âœ… Repository ingestion and code parsing
- âœ… File relationship discovery (imports, dependencies)
- âœ… Impact analysis (who depends on this file?)
- âœ… Context packing for AI assistants
- âœ… Full-text search on file paths and content
- âŒ Memory Store
- âŒ Knowledge RAG
- âŒ Auto-extraction

**Use When**:

- You want code navigation and analysis only
- You don't need LLM-powered features
- You're working in air-gapped environments
- You want minimal resource usage

[â†’ Minimal Deployment Guide](minimal.md){ .md-button .md-button--primary }

---

### Standard - Code Graph + Memory

**Perfect for**: Teams building project knowledge bases

```yaml
Requirements:
  - Neo4j database
  - Docker & docker-compose
  - Embedding model (Ollama/OpenAI/Gemini) âœ“
  - No LLM needed âœ“

Resources:
  - Image size: ~600MB
  - Memory: ~2GB RAM
  - Startup time: ~8 seconds
```

**Available Features**:

- âœ… All Minimal features
- âœ… Manual memory management (add/update/delete)
- âœ… Vector-based memory search
- âœ… Project memory summaries
- âœ… Memory superseding (track decision changes)
- âŒ Auto-extraction from git/conversations
- âŒ Knowledge RAG

**Use When**:

- You want to maintain project decision logs
- You need searchable team knowledge
- You have access to an embedding service
- You prefer manual curation over auto-extraction

[â†’ Standard Deployment Guide](standard.md){ .md-button .md-button--primary }

---

### Full - All Features

**Perfect for**: Teams wanting complete AI-powered capabilities

```yaml
Requirements:
  - Neo4j database
  - Docker & docker-compose
  - LLM (Ollama/OpenAI/Gemini/OpenRouter) âœ“
  - Embedding model âœ“

Resources:
  - Image size: ~800MB
  - Memory: ~4GB RAM (+ LLM requirements)
  - Startup time: ~15 seconds
```

**Available Features**:

- âœ… All Standard features
- âœ… Automatic memory extraction from:
    - Git commits
    - AI conversations
    - Code comments (TODO/FIXME/NOTE)
    - Q&A sessions
- âœ… Knowledge base RAG:
    - Document ingestion
    - Intelligent Q&A
    - Multi-format support
- âœ… Batch repository analysis

**Use When**:

- You want fully automated knowledge extraction
- You need document Q&A capabilities
- You have LLM infrastructure available
- You want maximum AI assistance

[â†’ Full Deployment Guide](full.md){ .md-button .md-button--primary }

---

## ğŸ”„ Mode Comparison Matrix

| Feature Category | Minimal | Standard | Full |
|------------------|---------|----------|------|
| **Code Graph** |
| Repository ingestion | âœ… | âœ… | âœ… |
| Incremental updates | âœ… | âœ… | âœ… |
| File search | âœ… | âœ… | âœ… |
| Impact analysis | âœ… | âœ… | âœ… |
| Context packing | âœ… | âœ… | âœ… |
| **Memory Store** |
| Add memory | âŒ | âœ… | âœ… |
| Search memories | âŒ | âœ… (vector) | âœ… (vector) |
| Update/delete | âŒ | âœ… | âœ… |
| Supersede | âŒ | âœ… | âœ… |
| Extract from git | âŒ | âŒ | âœ… (LLM) |
| Extract from chat | âŒ | âŒ | âœ… (LLM) |
| Extract from code | âŒ | âŒ | âœ… (LLM) |
| **Knowledge RAG** |
| Add documents | âŒ | âŒ | âœ… |
| Query knowledge | âŒ | âŒ | âœ… (LLM) |
| Vector search | âŒ | âŒ | âœ… |
| **Infrastructure** |
| Neo4j | Required | Required | Required |
| Embedding | - | Required | Required |
| LLM | - | - | Required |
| **Performance** |
| Image size | 500MB | 600MB | 800MB |
| RAM usage | 1GB | 2GB | 4GB+ |
| Startup time | 5s | 8s | 15s |

## ğŸ—ï¸ Architecture Diagrams

### Minimal Mode Architecture

```mermaid
graph TB
    subgraph "Client"
        A[Claude Desktop / API Client]
    end

    subgraph "Docker Network"
        B[MCP Server<br/>Minimal]
        C[(Neo4j<br/>Graph DB)]
    end

    subgraph "Code Graph Services"
        D[Code Ingestor]
        E[Graph Service]
        F[Ranker]
        G[Pack Builder]
    end

    A -->|MCP/REST| B
    B --> D
    B --> E
    B --> F
    B --> G
    D -->|Store| C
    E -->|Query| C

    style B fill:#90EE90
    style C fill:#87CEEB
```

### Standard Mode Architecture

```mermaid
graph TB
    subgraph "Client"
        A[Claude Desktop / API Client]
    end

    subgraph "Docker Network"
        B[MCP Server<br/>Standard]
        C[(Neo4j<br/>Graph DB)]
    end

    subgraph "Code Graph Services"
        D[Code Ingestor]
        E[Graph Service]
    end

    subgraph "Memory Services"
        F[Memory Store]
    end

    subgraph "External"
        G[Embedding Service<br/>Ollama/OpenAI]
    end

    A -->|MCP/REST| B
    B --> D
    B --> E
    B --> F
    D -->|Store| C
    E -->|Query| C
    F -->|Store/Search| C
    F -->|Vectorize| G

    style B fill:#FFD700
    style C fill:#87CEEB
    style G fill:#FFA07A
```

### Full Mode Architecture

```mermaid
graph TB
    subgraph "Client"
        A[Claude Desktop / API Client]
    end

    subgraph "Docker Network"
        B[MCP Server<br/>Full]
        C[(Neo4j<br/>Graph DB)]
        D[Ollama<br/>Optional]
    end

    subgraph "All Services"
        E[Code Graph]
        F[Memory Store]
        G[Knowledge RAG]
        H[Memory Extractor]
    end

    subgraph "External/Optional"
        I[LLM Service<br/>OpenAI/Gemini]
        J[Embedding Service]
    end

    A -->|MCP/REST| B
    B --> E
    B --> F
    B --> G
    B --> H
    E -->|Store| C
    F -->|Store/Search| C
    G -->|Store/Query| C
    F -->|Vectorize| J
    G -->|Generate| I
    H -->|Analyze| I

    D -.->|Local LLM| I
    D -.->|Local Embed| J

    style B fill:#FF6347
    style C fill:#87CEEB
    style D fill:#DDA0DD
```

## ğŸš€ Quick Decision Guide

Use this flowchart to choose your deployment mode:

```mermaid
graph TD
    A[Start] --> B{Do you need<br/>LLM features?}
    B -->|No| C{Do you need<br/>memory search?}
    B -->|Yes| D[Full Mode]
    C -->|No| E[Minimal Mode]
    C -->|Yes| F{Can you provide<br/>embedding service?}
    F -->|Yes| G[Standard Mode]
    F -->|No| E

    E --> H[âœ“ Code Graph only<br/>âœ“ No external deps<br/>âœ“ Fast & lightweight]
    G --> I[âœ“ Code Graph<br/>âœ“ Memory Store<br/>âš  Need embedding]
    D --> J{Do you have<br/>local GPU?}
    J -->|Yes| K[Use with-ollama profile]
    J -->|No| L[Use cloud LLM]
    K --> M[âœ“ All features<br/>âœ“ Self-hosted<br/>âš  High resources]
    L --> N[âœ“ All features<br/>âœ“ Lower resources<br/>âš  API costs]

    style E fill:#90EE90
    style G fill:#FFD700
    style K fill:#FF6347
    style L fill:#FF6347
```

## ğŸ“‹ Pre-Deployment Checklist

### For All Modes

- [ ] Docker installed (version 20.10+)
- [ ] docker-compose installed (version 1.29+)
- [ ] At least 4GB free disk space
- [ ] Ports 7474, 7687, 8000 available
- [ ] `.env` file configured

### Additional for Standard Mode

- [ ] Embedding service available:
    - [ ] Local Ollama running, or
    - [ ] OpenAI API key, or
    - [ ] Google API key for Gemini

### Additional for Full Mode

- [ ] LLM service available:
    - [ ] Local Ollama running, or
    - [ ] OpenAI API key, or
    - [ ] Google API key, or
    - [ ] OpenRouter API key
- [ ] Embedding service (same as Standard)
- [ ] For local Ollama: GPU with 8GB+ VRAM (optional but recommended)

## ğŸ”„ Switching Between Modes

You can switch deployment modes at any time. Data in Neo4j is preserved.

```bash
# Stop current deployment
make docker-stop

# Start different mode
make docker-minimal    # or
make docker-standard   # or
make docker-full
```

!!! warning "Configuration Required"
    When switching to Standard or Full mode, update your `.env` file with required API keys and service URLs.

## ğŸ“š Next Steps

- [Minimal Deployment Guide](minimal.md)
- [Standard Deployment Guide](standard.md)
- [Full Deployment Guide](full.md)
- [Docker Guide](docker.md)
